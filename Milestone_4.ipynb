{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Milestone 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikFloden/Art-Style-Transfer-Using-Neural-Networks/blob/main/Milestone_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEFu8EUKH6Ti"
      },
      "source": [
        "# Milestone 4 - Implementing Neural Style Transfer\n",
        "\n",
        "__Objective__: Use Artificial Intelligence to create art using the Neural Style Transfer algorithm that was introduced by Gatys et al. in their 2015 paper titled, ‘A Neural Algorithm of Artistic Style’. We will be applying the artistic style from an image (we’ll call this the style image) to another image (our content image). We will be performing this style transfer using a 3-component loss function that includes the Content Loss, Style Loss, and the Total Variation Loss\n",
        "\n",
        "__Notes__:\n",
        "- Recall from earlier lessons that within the space of a pretrained CNN, there is stored ‘knowledge’ known as the latent space. The filters of pretrained CNNs are hierarchical learners which means the lower layers store information relating to simple local information such as blobs, colors, edges, etc. Mid-tier layers capture a combination of things from lower layers to recognize corners and simple shapes. The upper layers capture more complex patterns and abstract features. \n",
        "- It is advised you use Keras instead of the Keras included with TensorFlow 2.0.\n",
        "- Use the requirements.txt file attached\n",
        "\n",
        "\n",
        "__Workflow__:\n",
        "\n",
        "1. Let’s start developing the intuition for our loss functions. We first need to develop a Content Loss function. \n",
        "    - In order to preserve the contour lines and spatial layouts of our content image, if we propagate our image through a pretrained CNN and look at the activations of the upper or higher layers, it should activate on well defined/recognizable abstract qualities in the image. \n",
        "    - You will need to compute the activation of a specific upper layer, for instance, if using VGG19 (recommended), you can utilize the filter named, `block4_conv2` for both your content and style images.\n",
        "    - Compute the L2-norm (sum of squared differences) between these activations. The content loss is the L2-norm between the features of our input image and the features of the generated, output image.\n",
        "    - Our Content Loss function’s aim is to ensure the output generated image will have some similarity to the content image.\n",
        "2. Let’s develop the intuition behind our Style loss. Instead of a single upper layer, we will utilize multiple layers of our pretrained CNN to obtain our style loss function. This is because we wish to capture multi-scale representations and textures from our style image. This allows us to capture the local style and avoid capturing global arrangements.\n",
        "    - We use multiple layers for example (if using VGG19) `['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'Block5_conv1']\n",
        "    - Our goal during training is to minimize the loss between the style of our generated image and the style of our style image. This ensures that the style of our generated images is correlated to the style of the style image. \n",
        "    - In order to build this style loss function, we need to compute the correlations between the activation layers of our selected CNN filters. To do this we compute the Gram Matrix between the activations of these layers. A Gram matrix is the inner product of a set of features maps.\n",
        "3. Build a function that returns the Gram Matrix. The Gram Matrix in our case is the dot product between the input vectors (feature maps) and their transpose. This can be built by:\n",
        "    - Flattening the input features using `features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))`\n",
        "    - Then finding the dot product with its transpose. This can be done with the Keras backend function shown here: `gram = K.dot(features, K.transpose(features))`\n",
        "    - Computing the L2 norm allows us to force our generated output image to have similar style characteristics, but not necessarily the same structural contents, as the style image.\n",
        "4. We finally get to our third loss function, the Total-Variation loss which now operates only on the output image. This function was not part of the original paper by Gatys et al. but was introduced later on due to better spatial smoothness in the output image (i.e. more locally coherent).  It’s effectively a measure of noise in an image and thus by lowering its loss, we make more aesthetically appealing images. To obtain the Total Variation loss, you will need to:\n",
        "    - Shift the image one pixel to the right and calculate the sum of squared error between the transferred and original. \n",
        "    - To square our tensors we can use the Keras backend function `k.square` and to get the sum we can use `k.sum`\n",
        "    - Then we do this again by we shift the image one pixel down this time\n",
        "5. Now you can combine all three loss functions to obtain one single loss function to minimize:\n",
        "    - This is given as `total_loss = [style(style_image) - style(generated_image)] + [content(original_image) - content(generated_image)] + total_variation_loss`\n",
        "6. Now that you have loss functions, let’s start putting together some helper utility functions to implement Neural Style Transfer\n",
        "    - Create a `pre_process_image()` function that takes the image or input path to an image as its argument and outputs an image using the `vgg19.preprocess_input()` function.\n",
        "    - Create a `deprocess_image()` function that removes the zero-center by mean pixel and clips the output values between 0 and 255\n",
        "7. Define two Keras variables to store our content and style images to get the tensor representations of our images.\n",
        "    - Use the Keras backend function `K.variable` to get the tensor representation\n",
        "    - Create a placeholder for our generated output image using `K.placeholder((1, img_nrows, img_ncols, 3))`\n",
        "8. Combine the three images into one single input tensor using `K.concatenate()`.\n",
        "9. Load the pretrained (imagenet)  VGG19 network using our input tensor (i.e. our 3 images) as the input without loading the top of the network.\n",
        "10. Select your network layer you will be using for your Content Loss.  We use upper layers so that high-level features are represented so selecting `block5_conv2` for instance would be a good choice.  Get the symbolic outputs of each key layer like this:\n",
        "    - `outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])`\n",
        "    - Get the single  `layer_features = outputs_dict['block5_conv2']`\n",
        "11. Now we can extract the features from the layer that we chose from the input tensor like this:\n",
        "    - `content_image_features = layer_features[0, :, :, :]`\n",
        "    - `combined_features = layer_features[2, :, :, :]`\n",
        "12. Finally, we loop through these feature_layers to calculate the style loss.\n",
        "13. We now need to create a way to get the gradients of the generated image with respect to the loss.\n",
        "    - You can use Keras’s `K.gradients` and `k.function` to build this.\n",
        "    - After which create a simple function called `eval_loss_and_grads` that will  return the loss and gradients\n",
        "\n",
        "14. Create a class called `evaluator` that contains methods that calculate the overall loss and gradients as described previously. This is needed so that we can compute our  loss and gradients in one pass while retrieving them via two separate functions, `loss` and `grads`. This is done because scipy.optimize  requires separate functions for loss and gradients, but computing them separately would be inefficient.\n",
        "\n",
        "15. All our building blocks are now in place to implement the Neural Style Transfer Algorithm. However, we can fine-tune the weightings of the contribution of the style and content images by using some weighting parameters. These are multiplied by each type of loss. They are the `content_weight`, `total_variation_weight` and the `style_weight`.\n",
        "\n",
        "16. Start iteratively minimize our total loss function using the scipy-based optimization method called `scipy.optimize.fmin_l_bfgs_b`.\n",
        "    - To be track on this training process for each iteration print out some logging information such as the Iteration no. and  the loss \n",
        "    - Display your Neural Style Transfer generated image the end\n",
        "17. Experiment with different weighting combinations, different layers, number of iterations and of course changing your content and style images to create amazing AI-generated art using NST!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3cO7_fYQoPF"
      },
      "source": [
        "# Import our modules. Note we use keras and not the TensorFlow2.0 keras\n",
        "import keras as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "# Import our VGG19 model\n",
        "from tensorflow.keras.applications import vgg16 as vgg\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrQh0uvk5Uff"
      },
      "source": [
        "### Download our style and base images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF9xfCWw4Kz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bb16ae-1631-433e-98fc-567521e5544d"
      },
      "source": [
        "!wget -O style_image.jpg https://i.imgur.com/GwoGyMl.jpg #Download Features Style Image\n",
        "#!wget -O style_image.jpg https://i.imgur.com/UkgSWFV.jpg #Download Candy Style Image\n",
        "#!wget -O style_image.jpg https://i.imgur.com/ivOAEV1.jpg #Download Mosaic Style Image\n",
        "\n",
        "!wget -O base_image.jpg https://i.imgur.com/UCDA6NR.jpg #Download Base Image\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-16 03:49:18--  https://i.imgur.com/GwoGyMl.jpg\n",
            "Resolving i.imgur.com (i.imgur.com)... 199.232.76.193\n",
            "Connecting to i.imgur.com (i.imgur.com)|199.232.76.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 321995 (314K) [image/jpeg]\n",
            "Saving to: ‘style_image.jpg’\n",
            "\n",
            "style_image.jpg     100%[===================>] 314.45K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-02-16 03:49:18 (9.73 MB/s) - ‘style_image.jpg’ saved [321995/321995]\n",
            "\n",
            "--2022-02-16 03:49:18--  https://i.imgur.com/UCDA6NR.jpg\n",
            "Resolving i.imgur.com (i.imgur.com)... 199.232.76.193\n",
            "Connecting to i.imgur.com (i.imgur.com)|199.232.76.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115057 (112K) [image/jpeg]\n",
            "Saving to: ‘base_image.jpg’\n",
            "\n",
            "base_image.jpg      100%[===================>] 112.36K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-02-16 03:49:18 (5.36 MB/s) - ‘base_image.jpg’ saved [115057/115057]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLO1SfjYSkmc"
      },
      "source": [
        "# Point to our image paths for our content/base image and style images\n",
        "base_image = PIL.Image.open(\"../content/base_image.jpg\")\n",
        "tf.keras.utils.get_file()\n",
        "style_image = PIL.Image.open(\"../content/style_image.jpg\")\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjIbF1wnTkXf"
      },
      "source": [
        "### These are the weights of the different loss components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXsRba6rVFYb"
      },
      "source": [
        "# Also, higher total_variation_weight implies higher spatial smoothness.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQZb4ENrTofR"
      },
      "source": [
        "#### Set the dimensions of the generated image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1wbT85eVXo5"
      },
      "source": [
        "# dimensions of the generated picture.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52O4XGPbTuMG"
      },
      "source": [
        "#### Preprocess Utility Function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPuOOpftVcYk"
      },
      "source": [
        "# util function to open, resize and format pictures into appropriate tensors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmEAwi4HTyhH"
      },
      "source": [
        "#### Deprocess Utility Function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV_BCp0QVfIz"
      },
      "source": [
        "def deprocess_image(x):\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvCzmq1AT06F"
      },
      "source": [
        "#### Convert out images to tensor representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u01VULGnVgqC"
      },
      "source": [
        "# get tensor representations of our images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM8SEwqXT4Pc"
      },
      "source": [
        "#### Examine what a tensor looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psOk6K8uhCr_"
      },
      "source": [
        "# Examine our tensor shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWI6EMgsT7eY"
      },
      "source": [
        "#### Create a blank placehold image to hold our output image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "697UVoBkVirx"
      },
      "source": [
        "# this will contain our generated image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcrRnUBRUDHj"
      },
      "source": [
        "#### Combine the 3 images into a single Keras tensor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDjwkxq0VkKd"
      },
      "source": [
        "# combine the 3 images into a single Keras tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyQMLC6rUJ1J"
      },
      "source": [
        "### Load our pretrained VGG19 without the head and the toplayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dYDaOI1VnNl"
      },
      "source": [
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "base_model = vgg.VGG16(weights='imagenet', \n",
        "                       include_top=False, \n",
        "                       input_shape=(48, 48, 3))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a5Z2QKIURW9"
      },
      "source": [
        "#### Understand how we get the layer names and how we create a dictionary of layer names as the key and the outputs of layer as the key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5gKI7B9hn9l"
      },
      "source": [
        "# We can extract the model names using the .name method to access it\n",
        "numlayer = len(base_model.layers)\n",
        "names = [base_model.layers[i].name for i in range(numlayer)]\n",
        "# Likewise you can do the same for the model output\n",
        "outputs = [base_model.layers[i].output for i in range(numlayer)]\n",
        "name_output_dict = dict(zip(names, outputs))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCEilp_0hgZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23423d51-779b-40c8-e563-fb840edba010"
      },
      "source": [
        "# Examine what the dictionary stores\n",
        "print(name_output_dict[])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 48, 48, 3), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPRxd2bQVTVP"
      },
      "source": [
        "# The Gram Matrix function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nib7uC3EVqlV"
      },
      "source": [
        "# compute the neural style loss\n",
        "# first we need to define 4 util functions\n",
        "\n",
        "# the gram matrix of an image tensor (feature-wise outer product)\n",
        "def gram_matrix(x):\n",
        "   assert K.ndim(x) == 3\n",
        "   features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "   gram = K.dot(features, K.transpose(features))\n",
        "   return gram"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJNhUkyHVXdW"
      },
      "source": [
        "# The Style Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zG_CWm6VlnM"
      },
      "source": [
        "# the \"style loss\" is designed to maintain\n",
        "# the style of the reference image in the generated image.\n",
        "# It is based on the gram matrices (which capture style) of\n",
        "# feature maps from the style reference image\n",
        "# and from the generated image\n",
        "def style_loss(style, combination):\n",
        "   assert K.ndim(style) == 3\n",
        "   assert K.ndim(combination) == 3\n",
        "   S = gram_matrix(style)\n",
        "   C = gram_matrix(combination)\n",
        "   channels = 3\n",
        "   size = img_nrows * img_ncols\n",
        "   return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43p8Bh1rVdpN"
      },
      "source": [
        "# The Content Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehrxsF5YVwm0"
      },
      "source": [
        "# an auxiliary loss function\n",
        "# designed to maintain the \"content\" of the\n",
        "# base image in the generated image\n",
        "def content_loss(base, combination):\n",
        "   return K.sum(K.square(combination - base))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX9OYVrPVhBt"
      },
      "source": [
        "# The Total Variation Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwQaZ99kVyGc"
      },
      "source": [
        "# the 3rd loss function, total variation loss,\n",
        "# designed to keep the generated image locally coherent\n",
        "def total_variation_loss(x):\n",
        "   assert K.ndim(x) == 4\n",
        "   a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "   b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "   return K.sum(K.pow(a + b, 1.25))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4EpcjQrRwlJ"
      },
      "source": [
        "## Select the Filter we'll be using for our Contentz Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxanGsbhVzyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e393398e-fabc-45a0-fbb4-6c1e43a70e6e"
      },
      "source": [
        "# combine these loss functions into a single scalar\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f5b0131ae393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# combine these loss functions into a single scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstyle_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstyle_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontent_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontent_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtotal_variation_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: style_loss() missing 1 required positional argument: 'combination'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgQ30yxdRGNP"
      },
      "source": [
        "## Select the filters you wish to use to build your Gram Matrix required for our Style Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHHVq7bEV1Iw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgzlGw02R294"
      },
      "source": [
        "### We loop through these feature_layers to calculate the style loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTsAS1qdV3E0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZgoaCklWrW_"
      },
      "source": [
        "#### We obtain the gradients of the generated image for the respective loss, and then use it to create a tensor function that returns the gradients "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZAc6j_qV4mA"
      },
      "source": [
        "# get the gradients of the generated image wrt the loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Fjohh5bg4a"
      },
      "source": [
        "#### We use this function in the Evaluator class defined below so that we can compute our  loss and gradients in one pass while retrieving them via two separate functions, `loss` and `grads`. This is done because scipy.optimize  requires separate functions for loss and gradients, but computing them separately would be inefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGcs7qXLWH9B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0yBAB8DV6JB"
      },
      "source": [
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAjG98_Sb2Z2"
      },
      "source": [
        "# We can now implement our Neural Style Transfer Algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M2eddv3ZBrA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify Iterations to run\n",
        "\n",
        "# Enlarge figure view when displaying final output\n",
        "\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "\n",
        "\n",
        "\n",
        "    # save current generated image after deprocessing\n",
        " \n",
        "\n",
        "# Display our images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDcNYeb-S43h"
      },
      "source": [
        "__Summary__:\n",
        "\n",
        "In this notebook we:\n",
        "- Loaded a pretrained (imagenet) VGG19 model without its top layer and used it to implement the Neural Style Transfer Algorithm.\n",
        "- We created 3 loss function, content loss, style loss, and total variance loss and combined them into a single loss function that was minimized in order to produce a generated image that copied the style of our style image onto our content image. \n",
        "\n",
        "__Deliverable__:\n",
        "\n",
        "The deliverable is a Jupyter Notebook documenting your workflow as you create your loss functions and then combine them using the recommended Keras functions. You are then to load your own content and style images and implement the Neural Style Transfer algorithm to create your own Art. Create a few different variations of your Art using different layers and weighting parameters. \n"
      ]
    }
  ]
}